# Yao Chen (陈瑶)

**Education**
* **Doctor of Philosophy in Computer Science**, Institute of Information Engineering, Chinese Academy of Sciences (2023 – 2028, expected)
* **Bachelor of Science in Computer Science**, Beijing Normal University (2019 – 2023)

**Email**: [chenyao2023@iie.ac.cn](mailto:chenyao2023@iie.ac.cn) / [chenyao23@mails.ucas.ac.cn](mailto:chenyao23@mails.ucas.ac.cn)

---

## About Me
I am a Doctoral student at the Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS), also affiliated with the University of Chinese Academy of Sciences (UCAS). My research interests focus on Large Language Models (LLMs), specifically in LLM Efficiency, Reasoning, and Dynamic Architectures.

---

## Publications
* **Improving Reasoning Capabilities in Small Models through Mixture-of-layers Distillation with Stepwise Attention on Key Information** Yao Chen, Jiawei Sheng, Wenyuan Zhang, Tingwen Liu  
  *In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)* [[Paper]](https://aclanthology.org/2025.emnlp-main.250/)  
  This work introduces a novel CoT distillation framework that transfers a teacher model's dynamic, stepwise attention on critical information to enhance a student model's reasoning capabilities. It employs a Mixture-of-Layers module to achieve adaptive weighted alignment between models with mismatched layer counts.

*(Note: Ongoing research projects are omitted to comply with double-blind review policies.)*

---
<p align="center">Last updated: Jan 2026</p>
