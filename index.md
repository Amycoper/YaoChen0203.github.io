
---
layout: page
title: Yao Chen (陈垚)
---

# Yao Chen (陈垚)

**PhD Student** [Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS)](http://www.iie.cas.cn/)  
[University of Chinese Academy of Sciences (UCAS)](https://www.ucas.ac.cn/)  
Email: `your_email@iie.ac.cn` | [Google Scholar](https://scholar.google.com/citations?user=PS_CX0AAAAAJ)

---

## About Me
I am a PhD student (Class of 2023) at the **Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS)**. My research interests focus on **Large Language Models (LLMs)**, specifically in:
* **LLM Efficiency**: Optimizing inference and training costs.
* **Reasoning**: Enhancing the logical capabilities of models.
* **Dynamic Architectures**: Exploring adaptive model structures.

---

## News
* **[2025.09]** Our work on **Chain-of-Thought Distillation** was accepted by **EMNLP 2025**!

---

## Publications

### Conference Papers
* **[EMNLP 2025]** **Chain-of-Thought Distillation for Large Language Models** **Yao Chen**, et al.  
  *[Link to Paper/ArXiv]*

*(Note: Ongoing research projects are omitted to comply with double-blind review policies.)*

---

## Education
* **PhD in Computer Science**, IIE, CAS (2023 – 2028, expected)
* **B.S. in Computer Science**, Beijing Normal University (2019 – 2023)

---
<p align="center">Last updated: Jan 2026</p>
