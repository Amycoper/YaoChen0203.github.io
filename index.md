# Yao Chen (陈瑶)

[cite_start]**PhD Student** [Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS)](http://www.iie.cas.cn/)   
[cite_start][University of Chinese Academy of Sciences (UCAS)](https://www.ucas.ac.cn/)   
Email: `chenyao2023@iie.ac.cn` / `chenyao23@mails.ucas.ac.cn` 

---

## Education
* [cite_start]**PhD in Computer Science**, IIE, CAS (2023 – 2028, expected) 
* **B.S. in Computer Science**, Beijing Normal University (2019 – 2023)

---

## About Me
[cite_start]I am a PhD student at the **Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS)**. My research interests focus on **Large Language Models (LLMs)**, specifically in **LLM Efficiency**, **Reasoning**, and **Dynamic Architectures**.

---

## Publications
* [cite_start]**Improving Reasoning Capabilities in Small Models through Mixture-of-layers Distillation with Stepwise Attention on Key Information** **Yao Chen**, Jiawei Sheng, Wenyuan Zhang, Tingwen Liu   
  [cite_start]*In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (**EMNLP 2025**)* [[Paper]](https://aclanthology.org/2025.emnlp-main.250/) [cite: 30]  
  [cite_start]*This work introduces a novel CoT distillation framework that transfers a teacher model's dynamic, stepwise attention on critical information to enhance a student model's reasoning capabilities. It employs a Mixture-of-Layers module to achieve adaptive weighted alignment between models with mismatched layer counts.*

*(Note: Ongoing research projects are omitted to comply with double-blind review policies.)*

---
<p align="center">Last updated: Jan 2026</p>
